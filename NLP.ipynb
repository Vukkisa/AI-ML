{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7429cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence.', 'NLTK', 'powerful', 'library', 'natural', 'language', 'processing.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/vj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#The NLTK stopwords corpus supports 23 languages. Here's the complete list:\n",
    "\n",
    "new=stopwords.words('english')\n",
    "#print(new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35695bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence.', 'NLTK', 'powerful', 'library', 'natural', 'language', 'processing.']\n"
     ]
    }
   ],
   "source": [
    "sentences = \"This is a sample sentence. NLTK is a powerful library for natural language processing.\"\n",
    "remove_stopwords = [word for word in sentences.split() if word.lower() not in stopwords.words('english')]\n",
    "print(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afd58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expression (regex) patterns to identify and remove unwanted characters or words from text data.\n",
    " #its present in library re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406201d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeninzation= process of converting a large paragraph into smaller parts called tokens. These tokens can be words, phrases, symbols, or even entire sentences.\n",
    "\n",
    "sent_tokenize= used to split a text into individual sentences.\n",
    "\n",
    "word_tokenize= used to split a text into individual words or tokens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25664c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.991025447845459}]\n"
     ]
    }
   ],
   "source": [
    "# Ensure `transformers` is available, install into the current Python environment if needed\n",
    "import sys, subprocess\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "except Exception:\n",
    "    print(\"`transformers` not found — installing into the current Python environment...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'])\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers'])\n",
    "    from transformers import pipeline\n",
    "\n",
    "# Create sentiment classifier and run it\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "result = classifier(\"I love using transformers library for natural language processing tasks.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b3f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model.save_pretrained(\"./vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vj/Desktop/AI-ML/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "MPS (Metal Performance Shaders) available: True\n",
      "CPU only: False\n",
      "Using device: mps with dtype=float32\n",
      "\n",
      "Loading Stable Diffusion XL base model (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 19 files: 100%|██████████| 19/19 [43:59<00:00, 138.94s/it] \n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:08<00:00,  1.21s/it]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Safe image generation with Stable Diffusion XL\n",
    "# This cell checks for GPU/CPU, installs diffusers if needed, and handles resource constraints\n",
    "\n",
    "import sys, subprocess\n",
    "import torch\n",
    "\n",
    "# Install diffusers if not available\n",
    "try:\n",
    "    from diffusers import StableDiffusionXLPipeline\n",
    "except ImportError:\n",
    "    print(\"Installing diffusers...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'diffusers'])\n",
    "    from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "# Check device availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
    "cpu_only = not cuda_available and not mps_available\n",
    "\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "print(f\"MPS (Metal Performance Shaders) available: {mps_available}\")\n",
    "print(f\"CPU only: {cpu_only}\")\n",
    "\n",
    "# Device selection and dtype strategy\n",
    "if cuda_available:\n",
    "    device = \"cuda\"\n",
    "    dtype = torch.float16  # Use half precision on CUDA to save memory\n",
    "    print(f\"Using device: {device} with dtype=float16\")\n",
    "elif mps_available:\n",
    "    device = \"mps\"\n",
    "    dtype = torch.float32  # MPS generally works better with float32\n",
    "    print(f\"Using device: {device} with dtype=float32\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dtype = torch.float32\n",
    "    print(f\"Using device: {device} (CPU-only). Note: Stable Diffusion XL is very slow on CPU.\")\n",
    "    print(\"Consider using a smaller model or GPU for practical use.\")\n",
    "\n",
    "# Load and run the pipeline\n",
    "try:\n",
    "    print(\"\\nLoading Stable Diffusion XL base model (this may take a moment)...\")\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        torch_dtype=dtype\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate image\n",
    "    prompt = \"a realistic photo of a tiger in a forest\"\n",
    "    print(f\"\\nGenerating image with prompt: '{prompt}'\")\n",
    "    image = pipe(prompt, num_inference_steps=20).images[0]  # Reduce steps for faster generation\n",
    "    \n",
    "    # Save the image\n",
    "    output_path = \"/Users/vj/Desktop/AI-ML/tiger.png\"\n",
    "    image.save(output_path)\n",
    "    print(f\"Image saved to: {output_path}\")\n",
    "    print(f\"Image size: {image.size}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during image generation: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"- On CPU: Stable Diffusion XL is impractically slow. Consider using a smaller model or GPU.\")\n",
    "    print(\"- Memory issues: Try using a smaller model (e.g., 'stabilityai/stable-diffusion-2-1') or enable memory-efficient features.\")\n",
    "    print(\"- GPU out of memory: Consider reducing num_inference_steps or using a quantized model.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0de406be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'how', 'are', 'you', '?']\n",
      "[7592, 1010, 2129, 2024, 2017, 1029]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(\"Model:\", model)\\nprint(\"Tokens:\", tokens)\\nprint(\"Input keys:\", list(inputs.keys()))\\nprint(\"Input tensor shapes:\")\\nfor k, v in inputs.items():\\n    print(f\"  {k}: {tuple(v.shape)}\")\\nprint(\"\\nFull inputs:\")\\nprint(inputs)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "text = \"Hello, how are you?\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "tk_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens)\n",
    "print(tk_ids)\n",
    "#print(inputs)\n",
    "\n",
    "'''print(\"Model:\", model)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Input keys:\", list(inputs.keys()))\n",
    "print(\"Input tensor shapes:\")\n",
    "for k, v in inputs.items():\n",
    "    print(f\"  {k}: {tuple(v.shape)}\")\n",
    "print(\"\\nFull inputs:\")\n",
    "print(inputs)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4aafa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- IGNORE ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- IGNORE ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
